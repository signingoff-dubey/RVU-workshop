{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38d04c9",
   "metadata": {},
   "source": [
    "# House Price Prediction Project\n",
    "\n",
    "This notebook demonstrates a complete machine learning pipeline for predicting house prices using the California Housing dataset. We'll go through the following steps:\n",
    "\n",
    "1. Data Loading and Preprocessing\n",
    "2. Exploratory Data Analysis\n",
    "3. Feature Engineering\n",
    "4. Model Selection and Training\n",
    "5. Model Evaluation\n",
    "6. Hyperparameter Tuning\n",
    "7. Making Predictions\n",
    "\n",
    "Let's begin by importing the necessary libraries and loading our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8771e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the style for our plots\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae260ae",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "We'll use the California Housing dataset from scikit-learn. This dataset contains information about housing prices in California, with various features such as:\n",
    "- MedInc: Median income in the block\n",
    "- HouseAge: Median house age in the block\n",
    "- AveRooms: Average number of rooms per household\n",
    "- AveBedrms: Average number of bedrooms per household\n",
    "- Population: Block population\n",
    "- AveOccup: Average occupancy\n",
    "- Latitude: Block latitude\n",
    "- Longitude: Block longitude\n",
    "- Target: Median house value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the data\n",
    "housing = fetch_california_housing()\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y = housing.target\n",
    "\n",
    "# Display the first few rows and basic information about the dataset\n",
    "print(\"Dataset Shape:\", X.shape)\n",
    "print(\"\\nFeature Names:\", housing.feature_names)\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(X.head())\n",
    "print(\"\\nBasic statistics of the target variable (house prices in $100,000):\")\n",
    "print(pd.Series(y).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f6db3",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Let's analyze our dataset through visualizations and statistical summaries to better understand the relationships between variables and identify any patterns or anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84256f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(X.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y, bins=50)\n",
    "plt.title('Distribution of House Prices')\n",
    "plt.xlabel('Price (in $100,000)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Create scatter plots for important features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "sns.scatterplot(data=X, x='MedInc', y=y, ax=axes[0])\n",
    "axes[0].set_title('House Price vs. Median Income')\n",
    "\n",
    "sns.scatterplot(data=X, x='HouseAge', y=y, ax=axes[1])\n",
    "axes[1].set_title('House Price vs. House Age')\n",
    "\n",
    "sns.scatterplot(data=X, x='AveRooms', y=y, ax=axes[2])\n",
    "axes[2].set_title('House Price vs. Average Rooms')\n",
    "\n",
    "sns.scatterplot(data=X, x='Population', y=y, ax=axes[3])\n",
    "axes[3].set_title('House Price vs. Population')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68174ca0",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Now that we understand our data better, let's prepare it for modeling by:\n",
    "1. Scaling numerical features\n",
    "2. Creating new features\n",
    "3. Handling any outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f031b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "X['RoomsByBedrooms'] = X['AveRooms'] / X['AveBedrms']\n",
    "X['PopulationByHousehold'] = X['Population'] / X['AveOccup']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Display the first few rows of scaled features\n",
    "print(\"Scaled features:\")\n",
    "print(X_scaled.head())\n",
    "\n",
    "# Check for and remove outliers using IQR method\n",
    "def remove_outliers(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    for column in columns:\n",
    "        Q1 = df_clean[column].quantile(0.25)\n",
    "        Q3 = df_clean[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_clean = df_clean[\n",
    "            (df_clean[column] >= lower_bound) & \n",
    "            (df_clean[column] <= upper_bound)\n",
    "        ]\n",
    "    return df_clean\n",
    "\n",
    "# Remove outliers from selected columns\n",
    "columns_to_clean = ['MedInc', 'AveRooms', 'Population']\n",
    "X_clean = remove_outliers(X_scaled, columns_to_clean)\n",
    "y_clean = y[X_clean.index]\n",
    "\n",
    "print(\"\\nShape before outlier removal:\", X_scaled.shape)\n",
    "print(\"Shape after outlier removal:\", X_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e2f04",
   "metadata": {},
   "source": [
    "## 4. Model Selection and Training\n",
    "\n",
    "We'll try two different models and compare their performance:\n",
    "1. Linear Regression (baseline model)\n",
    "2. Random Forest Regressor (more complex model)\n",
    "\n",
    "Let's split our data into training and testing sets, then train both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcab196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clean, y_clean, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with both models\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Print initial results\n",
    "print(\"Linear Regression Results:\")\n",
    "print(\"R² Score:\", r2_score(y_test, lr_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, lr_pred)))\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "print(\"R² Score:\", r2_score(y_test, rf_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, rf_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a1eca3",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "Let's evaluate our models in more detail by:\n",
    "1. Comparing predicted vs actual values\n",
    "2. Analyzing residuals\n",
    "3. Identifying feature importance (for Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44cb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots of predicted vs actual values\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Linear Regression\n",
    "ax1.scatter(y_test, lr_pred, alpha=0.5)\n",
    "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax1.set_xlabel('Actual Price')\n",
    "ax1.set_ylabel('Predicted Price')\n",
    "ax1.set_title('Linear Regression: Predicted vs Actual')\n",
    "\n",
    "# Random Forest\n",
    "ax2.scatter(y_test, rf_pred, alpha=0.5)\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax2.set_xlabel('Actual Price')\n",
    "ax2.set_ylabel('Predicted Price')\n",
    "ax2.set_title('Random Forest: Predicted vs Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot feature importance for Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Linear Regression residuals\n",
    "residuals_lr = y_test - lr_pred\n",
    "ax1.scatter(lr_pred, residuals_lr, alpha=0.5)\n",
    "ax1.axhline(y=0, color='r', linestyle='--')\n",
    "ax1.set_xlabel('Predicted Price')\n",
    "ax1.set_ylabel('Residuals')\n",
    "ax1.set_title('Linear Regression: Residual Plot')\n",
    "\n",
    "# Random Forest residuals\n",
    "residuals_rf = y_test - rf_pred\n",
    "ax2.scatter(rf_pred, residuals_rf, alpha=0.5)\n",
    "ax2.axhline(y=0, color='r', linestyle='--')\n",
    "ax2.set_xlabel('Predicted Price')\n",
    "ax2.set_ylabel('Residuals')\n",
    "ax2.set_title('Random Forest: Residual Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b5d23a",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning\n",
    "\n",
    "Since the Random Forest model performed better, let's optimize its hyperparameters using GridSearchCV to improve its performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"Best parameters:\", rf_grid.best_params_)\n",
    "print(\"Best score:\", np.sqrt(-rf_grid.best_score_))\n",
    "\n",
    "# Make predictions with the optimized model\n",
    "rf_best_pred = rf_grid.predict(X_test)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nOptimized Random Forest Results:\")\n",
    "print(\"R² Score:\", r2_score(y_test, rf_best_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, rf_best_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3e40f",
   "metadata": {},
   "source": [
    "## 7. Final Predictions and Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c9dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "model_filename = 'california_housing_model.joblib'\n",
    "joblib.dump(rf_grid.best_estimator_, model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "# Example of loading and using the model\n",
    "loaded_model = joblib.load(model_filename)\n",
    "\n",
    "# Make predictions on a sample\n",
    "sample_predictions = loaded_model.predict(X_test[:5])\n",
    "print(\"\\nSample predictions:\")\n",
    "print(\"Predicted prices:\", sample_predictions)\n",
    "print(\"Actual prices:\", y_test[:5].values)\n",
    "\n",
    "# Create a DataFrame with actual vs predicted values\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual Price': y_test[:5],\n",
    "    'Predicted Price': sample_predictions,\n",
    "    'Difference': y_test[:5].values - sample_predictions\n",
    "})\n",
    "print(\"\\nPrediction Comparison:\")\n",
    "print(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
